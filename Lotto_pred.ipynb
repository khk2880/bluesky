{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb3c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaina\\Documents\\Python Scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#현재 폴더 경로; 작업 폴더 기준\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1990801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'lottery.csv', 'Lotto_pred.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd754802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaina\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Jaina\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 train acc 0.018 loss 0.406 val acc nan loss nan\n",
      "epoch    1 train acc 0.013 loss 0.396 val acc nan loss nan\n",
      "epoch    2 train acc 0.016 loss 0.394 val acc nan loss nan\n",
      "epoch    3 train acc 0.015 loss 0.392 val acc nan loss nan\n",
      "epoch    4 train acc 0.011 loss 0.390 val acc nan loss nan\n",
      "epoch    5 train acc 0.015 loss 0.386 val acc nan loss nan\n",
      "epoch    6 train acc 0.022 loss 0.381 val acc nan loss nan\n",
      "epoch    7 train acc 0.027 loss 0.376 val acc nan loss nan\n",
      "epoch    8 train acc 0.039 loss 0.370 val acc nan loss nan\n",
      "epoch    9 train acc 0.044 loss 0.365 val acc nan loss nan\n",
      "epoch   10 train acc 0.051 loss 0.358 val acc nan loss nan\n",
      "epoch   11 train acc 0.061 loss 0.351 val acc nan loss nan\n",
      "epoch   12 train acc 0.074 loss 0.344 val acc nan loss nan\n",
      "epoch   13 train acc 0.080 loss 0.335 val acc nan loss nan\n",
      "epoch   14 train acc 0.089 loss 0.326 val acc nan loss nan\n",
      "epoch   15 train acc 0.102 loss 0.317 val acc nan loss nan\n",
      "epoch   16 train acc 0.102 loss 0.307 val acc nan loss nan\n",
      "epoch   17 train acc 0.108 loss 0.297 val acc nan loss nan\n",
      "epoch   18 train acc 0.118 loss 0.287 val acc nan loss nan\n",
      "epoch   19 train acc 0.114 loss 0.276 val acc nan loss nan\n",
      "epoch   20 train acc 0.118 loss 0.267 val acc nan loss nan\n",
      "epoch   21 train acc 0.124 loss 0.257 val acc nan loss nan\n",
      "epoch   22 train acc 0.134 loss 0.248 val acc nan loss nan\n",
      "epoch   23 train acc 0.135 loss 0.239 val acc nan loss nan\n",
      "epoch   24 train acc 0.134 loss 0.230 val acc nan loss nan\n",
      "epoch   25 train acc 0.130 loss 0.220 val acc nan loss nan\n",
      "epoch   26 train acc 0.135 loss 0.212 val acc nan loss nan\n",
      "epoch   27 train acc 0.136 loss 0.204 val acc nan loss nan\n",
      "epoch   28 train acc 0.125 loss 0.197 val acc nan loss nan\n",
      "epoch   29 train acc 0.130 loss 0.190 val acc nan loss nan\n",
      "epoch   30 train acc 0.139 loss 0.182 val acc nan loss nan\n",
      "epoch   31 train acc 0.132 loss 0.177 val acc nan loss nan\n",
      "epoch   32 train acc 0.141 loss 0.170 val acc nan loss nan\n",
      "epoch   33 train acc 0.132 loss 0.169 val acc nan loss nan\n",
      "epoch   34 train acc 0.150 loss 0.160 val acc nan loss nan\n",
      "epoch   35 train acc 0.140 loss 0.154 val acc nan loss nan\n",
      "epoch   36 train acc 0.162 loss 0.148 val acc nan loss nan\n",
      "epoch   37 train acc 0.138 loss 0.142 val acc nan loss nan\n",
      "epoch   38 train acc 0.161 loss 0.136 val acc nan loss nan\n",
      "epoch   39 train acc 0.138 loss 0.132 val acc nan loss nan\n",
      "epoch   40 train acc 0.147 loss 0.127 val acc nan loss nan\n",
      "epoch   41 train acc 0.155 loss 0.123 val acc nan loss nan\n",
      "epoch   42 train acc 0.144 loss 0.118 val acc nan loss nan\n",
      "epoch   43 train acc 0.140 loss 0.117 val acc nan loss nan\n",
      "epoch   44 train acc 0.144 loss 0.113 val acc nan loss nan\n",
      "epoch   45 train acc 0.156 loss 0.108 val acc nan loss nan\n",
      "epoch   46 train acc 0.132 loss 0.107 val acc nan loss nan\n",
      "epoch   47 train acc 0.150 loss 0.096 val acc nan loss nan\n",
      "epoch   48 train acc 0.147 loss 0.095 val acc nan loss nan\n",
      "epoch   49 train acc 0.150 loss 0.091 val acc nan loss nan\n",
      "epoch   50 train acc 0.166 loss 0.088 val acc nan loss nan\n",
      "epoch   51 train acc 0.170 loss 0.088 val acc nan loss nan\n",
      "epoch   52 train acc 0.153 loss 0.081 val acc nan loss nan\n",
      "epoch   53 train acc 0.166 loss 0.077 val acc nan loss nan\n",
      "epoch   54 train acc 0.159 loss 0.074 val acc nan loss nan\n",
      "epoch   55 train acc 0.155 loss 0.073 val acc nan loss nan\n",
      "epoch   56 train acc 0.144 loss 0.074 val acc nan loss nan\n",
      "epoch   57 train acc 0.159 loss 0.072 val acc nan loss nan\n",
      "epoch   58 train acc 0.154 loss 0.065 val acc nan loss nan\n",
      "epoch   59 train acc 0.157 loss 0.063 val acc nan loss nan\n",
      "epoch   60 train acc 0.153 loss 0.060 val acc nan loss nan\n",
      "epoch   61 train acc 0.157 loss 0.060 val acc nan loss nan\n",
      "epoch   62 train acc 0.148 loss 0.064 val acc nan loss nan\n",
      "epoch   63 train acc 0.143 loss 0.056 val acc nan loss nan\n",
      "epoch   64 train acc 0.144 loss 0.057 val acc nan loss nan\n",
      "epoch   65 train acc 0.149 loss 0.054 val acc nan loss nan\n",
      "epoch   66 train acc 0.149 loss 0.051 val acc nan loss nan\n",
      "epoch   67 train acc 0.148 loss 0.052 val acc nan loss nan\n",
      "epoch   68 train acc 0.175 loss 0.047 val acc nan loss nan\n",
      "epoch   69 train acc 0.152 loss 0.047 val acc nan loss nan\n",
      "epoch   70 train acc 0.182 loss 0.052 val acc nan loss nan\n",
      "epoch   71 train acc 0.158 loss 0.048 val acc nan loss nan\n",
      "epoch   72 train acc 0.150 loss 0.043 val acc nan loss nan\n",
      "epoch   73 train acc 0.178 loss 0.044 val acc nan loss nan\n",
      "epoch   74 train acc 0.172 loss 0.040 val acc nan loss nan\n",
      "epoch   75 train acc 0.156 loss 0.039 val acc nan loss nan\n",
      "epoch   76 train acc 0.169 loss 0.035 val acc nan loss nan\n",
      "epoch   77 train acc 0.157 loss 0.036 val acc nan loss nan\n",
      "epoch   78 train acc 0.161 loss 0.038 val acc nan loss nan\n",
      "epoch   79 train acc 0.165 loss 0.035 val acc nan loss nan\n",
      "epoch   80 train acc 0.146 loss 0.035 val acc nan loss nan\n",
      "epoch   81 train acc 0.146 loss 0.035 val acc nan loss nan\n",
      "epoch   82 train acc 0.167 loss 0.031 val acc nan loss nan\n",
      "epoch   83 train acc 0.153 loss 0.029 val acc nan loss nan\n",
      "epoch   84 train acc 0.157 loss 0.031 val acc nan loss nan\n",
      "epoch   85 train acc 0.149 loss 0.033 val acc nan loss nan\n",
      "epoch   86 train acc 0.168 loss 0.029 val acc nan loss nan\n",
      "epoch   87 train acc 0.135 loss 0.033 val acc nan loss nan\n",
      "epoch   88 train acc 0.160 loss 0.029 val acc nan loss nan\n",
      "epoch   89 train acc 0.157 loss 0.027 val acc nan loss nan\n",
      "epoch   90 train acc 0.147 loss 0.027 val acc nan loss nan\n",
      "epoch   91 train acc 0.161 loss 0.026 val acc nan loss nan\n",
      "epoch   92 train acc 0.163 loss 0.027 val acc nan loss nan\n",
      "epoch   93 train acc 0.145 loss 0.028 val acc nan loss nan\n",
      "epoch   94 train acc 0.171 loss 0.025 val acc nan loss nan\n",
      "epoch   95 train acc 0.140 loss 0.023 val acc nan loss nan\n",
      "epoch   96 train acc 0.168 loss 0.023 val acc nan loss nan\n",
      "epoch   97 train acc 0.163 loss 0.026 val acc nan loss nan\n",
      "epoch   98 train acc 0.153 loss 0.025 val acc nan loss nan\n",
      "epoch   99 train acc 0.160 loss 0.021 val acc nan loss nan\n",
      "receive numbers\n",
      "0 : [3, 14, 16, 23, 37, 44]\n",
      "1 : [3, 4, 22, 27, 31, 38]\n",
      "2 : [3, 4, 14, 31, 32, 37]\n",
      "3 : [3, 4, 14, 23, 29, 31]\n",
      "4 : [3, 14, 15, 22, 29, 31]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "import numpy as np\n",
    "rows = np.loadtxt(\"lottery.csv\", delimiter=\",\")\n",
    "row_count = len(rows)\n",
    "\n",
    "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
    "def numbers2ohbin(numbers):\n",
    "\n",
    "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
    "\n",
    "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
    "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
    "    \n",
    "    return ohbin\n",
    "\n",
    "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
    "def ohbin2numbers(ohbin):\n",
    "\n",
    "    numbers = []\n",
    "    \n",
    "    for i in range(len(ohbin)):\n",
    "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
    "            numbers.append(i+1)\n",
    "    \n",
    "    return numbers\n",
    "    \n",
    "numbers = rows[:, 1:7]\n",
    "ohbins = list(map(numbers2ohbin, numbers))\n",
    "\n",
    "x_samples = ohbins[0:row_count-1]\n",
    "y_samples = ohbins[1:row_count]\n",
    "# 데이터 나누기\n",
    "train_idx = (0, 1028)\n",
    "# val_idx = (1027, 1028)\n",
    "# test_idx = (1029, len(x_samples))\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# 모델을 정의합니다.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
    "    keras.layers.Dense(45, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 매 에포크마다 훈련과 검증의 손실 및 정확도를 기록하기 위한 변수\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "# 최대 100번 에포크까지 수행\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    \n",
    "    for i in range(train_idx[0], train_idx[1]):\n",
    "        \n",
    "        xs = x_samples[i].reshape(1, 1, 45)\n",
    "        ys = y_samples[i].reshape(1, 45)\n",
    "        \n",
    "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
    "\n",
    "        batch_train_loss.append(loss)\n",
    "        batch_train_acc.append(acc)\n",
    "\n",
    "    train_loss.append(np.mean(batch_train_loss))\n",
    "    train_acc.append(np.mean(batch_train_acc))\n",
    "\n",
    "    batch_val_loss = []\n",
    "    batch_val_acc = []\n",
    "\n",
    "#     for i in range(val_idx[0], val_idx[1]):\n",
    "\n",
    "#         xs = x_samples[i].reshape(1, 1, 45)\n",
    "#         ys = y_samples[i].reshape(1, 45)\n",
    "        \n",
    "#         loss, acc = model.test_on_batch(xs, ys) #배치만큼 모델에 입력하여 나온 답을 정답과 비교함\n",
    "        \n",
    "#         batch_val_loss.append(loss)\n",
    "#         batch_val_acc.append(acc)\n",
    "\n",
    "#     val_loss.append(np.mean(batch_val_loss))\n",
    "#     val_acc.append(np.mean(batch_val_acc))\n",
    "\n",
    "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f} val acc {3:0.3f} loss {4:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss), np.mean(batch_val_acc), np.mean(batch_val_loss)))\n",
    "    # 번호 뽑기\n",
    "def gen_numbers_from_probability(nums_prob):\n",
    "\n",
    "    ball_box = []\n",
    "\n",
    "    for n in range(45):\n",
    "        ball_count = int(nums_prob[n] * 100 + 1)\n",
    "        ball = np.full((ball_count), n+1) #1부터 시작\n",
    "        ball_box += list(ball)\n",
    "\n",
    "    selected_balls = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        if len(selected_balls) == 6:\n",
    "            break\n",
    "        \n",
    "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
    "        ball = ball_box[ball_index]\n",
    "\n",
    "        if ball not in selected_balls:\n",
    "            selected_balls.append(ball)\n",
    "\n",
    "    return selected_balls\n",
    "    \n",
    "print('receive numbers')\n",
    "\n",
    "xs = x_samples[-1].reshape(1, 1, 45)\n",
    "\n",
    "ys_pred = model.predict_on_batch(xs)\n",
    "\n",
    "list_numbers = []\n",
    "\n",
    "for n in range(5):\n",
    "    numbers = gen_numbers_from_probability(ys_pred[0])\n",
    "    numbers.sort()\n",
    "    print('{0} : {1}'.format(n, numbers))\n",
    "    list_numbers.append(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aed3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
